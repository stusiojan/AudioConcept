{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AudioLIME Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b9c3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Trying to estimate tuning from empty frequency set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "def extract_full_features(y, sr):\n",
    "\n",
    "    features = []\n",
    "\n",
    "    # Chroma\n",
    "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "    features.extend([np.mean(chroma), np.var(chroma)])\n",
    "\n",
    "    # RMS\n",
    "    rms = librosa.feature.rms(y=y)[0]\n",
    "    features.extend([np.mean(rms), np.var(rms)])\n",
    "\n",
    "    # Spectral\n",
    "    features.extend([\n",
    "        np.var(librosa.feature.spectral_centroid(y=y, sr=sr)[0]),\n",
    "        np.var(librosa.feature.spectral_bandwidth(y=y, sr=sr)[0]),\n",
    "        np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr)[0]),\n",
    "        np.var(librosa.feature.spectral_rolloff(y=y, sr=sr)[0]),\n",
    "    ])\n",
    "\n",
    "    # Zero-crossing\n",
    "    zcr = librosa.feature.zero_crossing_rate(y)[0]\n",
    "    features.extend([np.mean(zcr), np.var(zcr)])\n",
    "\n",
    "    # Harmonic & Percussive\n",
    "    harmonic, percussive = librosa.effects.hpss(y)\n",
    "    features.extend([\n",
    "        np.mean(harmonic), np.var(harmonic),\n",
    "        np.mean(percussive), np.var(percussive)\n",
    "    ])\n",
    "\n",
    "    # Tempo\n",
    "    tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
    "    features.append(tempo)\n",
    "\n",
    "    # MFCCs\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)\n",
    "    for coeff in mfcc:\n",
    "        features.extend([np.mean(coeff), np.var(coeff)])\n",
    "\n",
    "    return features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path().resolve().parent))\n",
    "from audioLIME.audioLIME import lime_audio, factorization_spleeter\n",
    "from IPython.display import Audio, display\n",
    "import librosa\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audio\n",
    "file_path = \"../data/raw/blues.00000.wav\"\n",
    "y, sr = librosa.load(file_path, sr=22050)\n",
    "\n",
    "print(\"Audio:\", y.shape, y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "with open(\"../models/svm_genre_classifier.pkl\", \"rb\") as f:\n",
    "    model_bundle = pickle.load(f)\n",
    "\n",
    "scaler = model_bundle[\"scaler\"]\n",
    "svm = model_bundle[\"model\"]\n",
    "\n",
    "print(\"Scaler expects:\", scaler.n_features_in_)\n",
    "print(\"Single audio features shape:\",len(extract_full_features(y,sr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "import numpy as np\n",
    "\n",
    "def svm_predict(audio_input):\n",
    "    if isinstance(audio_input, np.ndarray):\n",
    "        if audio_input.ndim == 1:\n",
    "            audio_input = [audio_input]\n",
    "\n",
    "    features = []\n",
    "\n",
    "    for audio in audio_input:\n",
    "        if len(audio) < 2048:\n",
    "            audio = np.pad(audio, (0, 2048 - len(audio)))\n",
    "\n",
    "        f = extract_full_features(audio, sr)\n",
    "        features.append(f)\n",
    "\n",
    "    features = np.array(features)\n",
    "    scaled = scaler.transform(features)\n",
    "\n",
    "    if len(svm.classes_) == 2:\n",
    "        decision = svm.decision_function(scaled)\n",
    "        decision = np.vstack([-decision, decision]).T\n",
    "    else:\n",
    "        decision = svm.decision_function(scaled)\n",
    "\n",
    "    probs = softmax(decision, axis=1)\n",
    "    print(f\"[DEBUG] svm_predict returned shape: {probs.shape}\")\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_duration = len(y) / sr\n",
    "segment_length_seconds = 3.0\n",
    "\n",
    "n_segments = max(1, int(total_duration / segment_length_seconds))\n",
    "\n",
    "if y.ndim == 1:\n",
    "    y = np.stack([y, y], axis=-1)  # 2D shape (samples, channels)\n",
    "\n",
    "# Factorize\n",
    "factorizer = factorization_spleeter.SpleeterFactorization(\n",
    "    input=y,\n",
    "    temporal_segmentation_params={\n",
    "        \"type\": \"fixed_length\",\n",
    "        \"segment_length\": segment_length_seconds,\n",
    "        \"n_temporal_segments\": n_segments\n",
    "    },\n",
    "    composition_fn=np.sum,\n",
    "    target_sr=sr\n",
    ")\n",
    "\n",
    "print(f\"Segment length: {segment_length_seconds:.2f}s, Segments: {n_segments}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain with audioLIME\n",
    "explainer = lime_audio.LimeAudioExplainer(random_state=42)\n",
    "explanation = explainer.explain_instance(\n",
    "    factorization=factorizer,\n",
    "    predict_fn=svm_predict,\n",
    "    top_labels=1,\n",
    "    num_features=10,\n",
    "    num_samples=1000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "components, indices = explanation.get_sorted_components(\n",
    "    label=explanation.top_labels[0],\n",
    "    num_components=5,\n",
    "    return_indeces=True\n",
    ")\n",
    "\n",
    "for i, comp in enumerate(components):\n",
    "    print(f\"Component {indices[i]}\")\n",
    "    display(Audio(comp, rate=sr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
