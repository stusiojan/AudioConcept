{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90722800",
   "metadata": {},
   "source": [
    "# Original implementation of VGG 16 architecture\n",
    "\n",
    "3x3 kernel with padding of 1 and stride of 1.\n",
    "\n",
    "Input image resolution is 224x224 and is RGB image.\n",
    "\n",
    "Image resolution stays the same.\n",
    "\n",
    "Implementation based on Aladdin Persson VGG torch [tutorial](https://www.youtube.com/watch?v=ACmuBbuXn20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ed7ea7",
   "metadata": {},
   "source": [
    "# VGGish architecture for genre classification [paper](https://arxiv.org/pdf/1609.09430)\n",
    "\n",
    "\n",
    ">The only changes we made to VGG (configuration E) [2] were to\n",
    "the final layer (3087 units with a sigmoid) as well as the use of batch\n",
    "normalization instead of LRN. While the original network had 144M\n",
    "weights and 20B multiplies, the audio variant uses 62M weights and\n",
    "2.4B multiplies. We tried another variant that reduced the initial\n",
    "strides (as we did with AlexNet), but found that not modifying the\n",
    "strides resulted in faster training and better performance. With our\n",
    "setup, parallelizing beyond 10 GPUs did not help significantly, so\n",
    "we trained with 10 GPUs and 5 parameter servers.\n",
    "\n",
    "\n",
    "The model is originally trained on `YouTube-100M` dataset, which is much bigger than `GTZAN`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1017cfb",
   "metadata": {},
   "source": [
    "# GTZAN Audio Classification with VGGish Model\n",
    "\n",
    "I'm using pre-generated Mel spectrograms from the `GTZAN` images_original directory - not `YouTube-100M`\n",
    "\n",
    "Changes in VGG:\n",
    "- final layer - 3087 units with a sigmoid\n",
    "- batch normalization instead of LRN\n",
    "- 144M weights, 20B multiplies -> 62M weights, 2.4B multiplies\n",
    "- do not modify strides\n",
    "\n",
    "Optimized for macOS with ARM processors - Metal Performance Shaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17784e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage of pre-trained VGGish model\n",
    "\n",
    "import torch\n",
    "\n",
    "model = torch.hub.load('harritaylor/torchvggish', 'vggish')\n",
    "model.eval()\n",
    "\n",
    "# Download an example audio file\n",
    "import urllib\n",
    "url, filename = (\"http://soundbible.com/grab.php?id=1698&type=wav\", \"bus_chatter.wav\")\n",
    "try: urllib.URLopener().retrieve(url, filename)\n",
    "except: urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "model.forward(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb94ec57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn  # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7771c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integer values - number of channels in the convolutional layers\n",
    "# M - Maxpooling layer\n",
    "VGG16_architecture = [ \n",
    "    64, 64, \"M\", \n",
    "    128, 128, \"M\",\n",
    "    256, 256, 256, \"M\",\n",
    "    512, 512, 512, \"M\",\n",
    "    512, 512, 512, \"M\",\n",
    "    # Then flatten\n",
    "    # Then 4096x4096x1000 linear layers\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8376781",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGish(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=1000):\n",
    "        super(VGGish, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.conv_layers = self.create_conv_layers(VGG16_architecture)\n",
    "\n",
    "        # final layer - 3087 units with a sigmoid\n",
    "        self.fcs = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),   #7 = input size / 2^num_maxpool = 224 / 2^5\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"Input shape: {x.shape}\")\n",
    "        x = self.conv_layers(x)\n",
    "        print(f\"Shape after conv layers: {x.shape}\")\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        print(f\"Shape after flattening: {x.shape}\")\n",
    "        x = self.fcs(x)\n",
    "        print(f\"Shape after fully connected layers: {x.shape}\")\n",
    "        return x\n",
    "\n",
    "    def create_conv_layers(self, architecture):\n",
    "        layers = []\n",
    "        in_channels = self.in_channels\n",
    "\n",
    "        for x in architecture:\n",
    "            if type(x) == int:\n",
    "                out_channels = x\n",
    "\n",
    "                layers += [\n",
    "                    nn.Conv2d(\n",
    "                        in_channels=in_channels,\n",
    "                        out_channels=out_channels,\n",
    "                        kernel_size=(3, 3),\n",
    "                        stride=(1, 1),\n",
    "                        padding=(1, 1),\n",
    "                    ),\n",
    "                    nn.BatchNorm2d(x),  # Not included in the original paper\n",
    "                    nn.ReLU(),\n",
    "                ]\n",
    "                in_channels = x\n",
    "            elif x == \"M\":\n",
    "                layers += [nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))]\n",
    "\n",
    "        return nn.Sequential(*layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f48bed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 16, 224, 224])\n",
      "Input shape: torch.Size([3, 16, 224, 224])\n",
      "Shape after conv layers: torch.Size([3, 512, 7, 7])\n",
      "Shape after flattening: torch.Size([3, 25088])\n",
      "Shape after fully connected layers: torch.Size([3, 10])\n",
      "Input shape: torch.Size([3, 16, 224, 224])\n",
      "Shape after conv layers: torch.Size([3, 512, 7, 7])\n",
      "Shape after flattening: torch.Size([3, 25088])\n",
      "Shape after fully connected layers: torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "num_classes = 10\n",
    "model = VGGish(in_channels=16, num_classes=num_classes).to(device)\n",
    "BATCH_SIZE = 3\n",
    "x = torch.randn(3, 16, 224, 224).to(device)  # 3 images, 3 channels, 224x224\n",
    "print(x.shape)\n",
    "assert model(x).shape == torch.Size([BATCH_SIZE, num_classes])\n",
    "print(model(x).shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
